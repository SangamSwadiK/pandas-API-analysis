{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing of Jupyter Notebooks from Github\n",
    "\n",
    "Starting from the raw 1.2 million Jupyter notebooks, we convert the notebooks to scripts with `nbconvert`, then take the middle 50% of notebooks by size as our sample. From the resulting set of scripts we do a single token search for \"pandas\" to yield the resulting `converted_scripts.zip` files.\n",
    "\n",
    "Using the `converted_scripts/` set of around ~250k files, which can be downloaded at [here](https://drive.google.com/file/d/12M3n_gsejc1xmrFoAGwHUgFTpHIlfa2i/view?usp=sharing), we perform token search, then post-process the data for further analysis. This notebook begins at this step.\n",
    "\n",
    "Our end result should be a DataFrame where each row corresponds to a script, and there is a column for each token, indicating the number of times a token appears in a script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import os\n",
    "from collections import Counter, defaultdict\n",
    "from inspect import isclass, isfunction, ismodule\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import modin.pandas as mpd\n",
    "import pandas as pd\n",
    "import ray\n",
    "import regex\n",
    "\n",
    "ray.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate the set of search tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compile a list of pandas functions.\n",
    "\"\"\"\n",
    "\n",
    "# Get all the possible functions from these pandas classes and their subclasses.\n",
    "allowed_classes = [\n",
    "    pd,\n",
    "    pd.DataFrame,\n",
    "    pd.Series,\n",
    "    pd.io,\n",
    "    pd.core,\n",
    "    pd.Index,\n",
    "    pd.RangeIndex,\n",
    "    pd.CategoricalIndex,\n",
    "    pd.IntervalIndex,\n",
    "    pd.MultiIndex,\n",
    "    pd.IndexSlice,\n",
    "    pd.DatetimeIndex,\n",
    "    pd.TimedeltaIndex,\n",
    "    pd.PeriodIndex,\n",
    "    pd.Timestamp,\n",
    "    pd.Timedelta,\n",
    "    pd.DatetimeTZDtype,\n",
    "    pd.Period,\n",
    "    pd.Interval,\n",
    "    pd.Categorical,\n",
    "    pd.arrays,\n",
    "    pd.tseries,\n",
    "    pd.plotting,\n",
    "    pd.api,\n",
    "]\n",
    "classes = [(pd, \"pandas\")]\n",
    "\n",
    "functions = set()\n",
    "indexers = [\"iloc\", \"iat\", \"ix\", \"loc\", \"at\"]\n",
    "\n",
    "while classes:\n",
    "    obj, prefix = classes.pop()\n",
    "    for token in dir(obj):\n",
    "        t = getattr(obj, token)\n",
    "        # We do not consider unders, duners, or properties.\n",
    "        if token[0] == \"_\" or token[:2] == \"__\":\n",
    "            continue\n",
    "        elif isfunction(t):\n",
    "            functions.add(f\"{prefix}.{token}\")\n",
    "        elif isclass(t) or ismodule(t):\n",
    "            if (\n",
    "                prefix.count(\".\") > 5  # Prune search tree depth.\n",
    "                or t not in allowed_classes\n",
    "                or prefix.split(\".\")[-1] == token\n",
    "            ):\n",
    "                continue\n",
    "            classes.append((t, f\"{prefix}.{token}\"))\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "# Compute the set of unique function names.\n",
    "function_set = set([f.split(\".\")[-1] for f in functions])\n",
    "len(function_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "To reduce false positives, we block the following prefixes:\n",
    "    - numpy and matplotlib prefixes\n",
    "    - ' and \" for string functions, like format\n",
    "    - ] and } for list and dict types\n",
    "    \n",
    "Then we convert function names to regex tokens.\n",
    "\"\"\"\n",
    "\n",
    "blocked_prefixes = \"(?<!numpy|np|plt|matplotlib|\\\"|'|]|})\"\n",
    "function_token_set = {f\"{blocked_prefixes}\\.{f}\\(\" for f in function_set}\n",
    "indexer_token_set = {f\"\\.{indexer}\\[\" for indexer in indexers}\n",
    "pandas_token_set = {\"pd\", \"pandas\"}\n",
    "search_tokens_set = function_token_set | indexer_token_set | pandas_token_set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search each script file for search tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame of the python scripts.\n",
    "\n",
    "python_scripts = []\n",
    "scripts_dir = \"../../pandas-api-analysis-private/data/big_dataset/converted_scripts/\"\n",
    "\n",
    "for f in os.listdir(scripts_dir):\n",
    "    if not f.startswith(\".\") and not f.endswith(\"csv\"):\n",
    "        python_scripts.append(os.path.join(scripts_dir, f))\n",
    "\n",
    "python_scripts_df = mpd.DataFrame(python_scripts, columns=[\"script_path\"])\n",
    "python_scripts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the helper we apply to the DataFrame to parse a script for its tokens.\n",
    "\n",
    "\n",
    "def parse_file_tokens(file_path, search_tokens):\n",
    "    \"\"\"Parse the file and search for the desired regex expressions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path: str\n",
    "        File path to search for.\n",
    "    search_tokens: iterable\n",
    "        Regex expressions as an iterable.\n",
    "    \"\"\"\n",
    "    with open(file_path) as f:\n",
    "        contents = f.read()\n",
    "    search_tokens = regex.compile(\"|\".join(search_tokens))\n",
    "    return Counter(regex.findall(search_tokens, contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: this step will take > 10 min.\n",
    "# Use parse script to get a Counter of the tokens in each script.\n",
    "\n",
    "script_tokens_ser = python_scripts_df.apply(\n",
    "    lambda row: parse_file_tokens(row[\"script_path\"], search_tokens_set),\n",
    "    axis=\"columns\",\n",
    ")\n",
    "\n",
    "script_tokens_ser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine the scripts DataFrame and the new series.\n",
    "python_script_tokens_df = mpd.concat([python_scripts_df, script_tokens_ser], axis=1)\n",
    "python_script_tokens_df.rename({0: \"script_tokens\"}, axis=1, inplace=True)\n",
    "python_script_tokens_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transform the token counters to DF columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the display names of all search tokens.\n",
    "function_token_dnames = {f\".{f}(\" for f in function_set}\n",
    "indexer_token_dnames = {f\".{indexer}[\" for indexer in indexers}\n",
    "pandas_token_dnames = {\"pd\", \"pandas\"}\n",
    "all_tokens = function_token_dnames | indexer_token_dnames | pandas_token_dnames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define helper to get the count for a token.\n",
    "\n",
    "\n",
    "def get_token(row: str, token):\n",
    "    try:\n",
    "        return row[token]\n",
    "    except Exception:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warning: this step will take > 10 min.\n",
    "\n",
    "for token in all_tokens:\n",
    "    python_script_tokens_df[token] = python_script_tokens_df[\"script_tokens\"].apply(\n",
    "        lambda row: get_token(row, token)\n",
    "    )\n",
    "python_script_tokens_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm all column names are unique\n",
    "assert all(\n",
    "    [count == 1 for f, count in Counter(python_script_tokens_df.columns).items()]\n",
    "), \"Column names should be unique.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove irrevelant tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional tokens to exclude.\n",
    "exclude = set([\"pandas\", \"pd\"])\n",
    "exclude = exclude.intersection(set(python_script_tokens_df.columns))\n",
    "print(len(exclude))\n",
    "exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove these tokens from the df.\n",
    "python_script_tokens_df.drop(labels=exclude, inplace=True, axis=1)\n",
    "python_script_tokens_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with no tokens.\n",
    "token_count = python_script_tokens_df.iloc[:, 2:].sum(axis=1)\n",
    "token_count = token_count[token_count == 0].index\n",
    "token_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_script_tokens_df = python_script_tokens_df.drop(labels=token_count, axis=0)\n",
    "python_script_tokens_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# (Optional) Save to csv.\n",
    "python_script_tokens_df.to_csv(\"../data/filtered_token_breakdown.csv\", index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
