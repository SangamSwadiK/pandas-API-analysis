{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing of Jupyter Notebooks on Github\n",
    "\n",
    "Starting from a set of scripts that were converted from Jupyter notebooks, we perform Token search, then post-process the data for further analysis.\n",
    "\n",
    "Our end result should be a DataFrame where each row corresponds to a script, and there is a column for each Token, indicating the number of times a token appears in a script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import json\n",
    "import os\n",
    "from collections import defaultdict\n",
    "from inspect import isclass, isfunction, ismodule\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import modin.pandas as mpd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import ray\n",
    "import regex\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Compile a list of pandas functions recursively.\n",
    "\"\"\"\n",
    "\n",
    "# Get all the possible functions from these pandas classes and their subclasses.\n",
    "allowed_classes = [\n",
    "    pd,\n",
    "    pd.DataFrame,\n",
    "    pd.Series,\n",
    "    pd.io,\n",
    "    pd.core,\n",
    "    pd.Index,\n",
    "    pd.RangeIndex,\n",
    "    pd.CategoricalIndex,\n",
    "    pd.IntervalIndex,\n",
    "    pd.MultiIndex,\n",
    "    pd.IndexSlice,\n",
    "    pd.DatetimeIndex,\n",
    "    pd.TimedeltaIndex,\n",
    "    pd.PeriodIndex,\n",
    "    pd.Timestamp,\n",
    "    pd.Timedelta,\n",
    "    pd.DatetimeTZDtype,\n",
    "    pd.Period,\n",
    "    pd.Interval,\n",
    "    pd.Categorical,\n",
    "    pd.arrays,\n",
    "    pd.tseries,\n",
    "    pd.plotting,\n",
    "    pd.api,\n",
    "]\n",
    "classes = [(pd, \"pd\")]\n",
    "\n",
    "functions = set()\n",
    "indexers = [\"iloc\", \"iat\", \"ix\", \"loc\", \"at\"]\n",
    "\n",
    "while classes:\n",
    "    obj, prefix = classes.pop()\n",
    "    for token, t in vars(obj).items():\n",
    "        # We do not consider unders, duners, or properties.\n",
    "        if token[0] == \"_\" or token[:2] == \"__\":\n",
    "            continue\n",
    "        elif isfunction(t):\n",
    "            functions.add(f\"{prefix}.{token}\")\n",
    "        elif isclass(t) or ismodule(t):\n",
    "            if (\n",
    "                prefix.count(\".\") > 5 or t not in allowed_classes\n",
    "            ):  # Prune search tree depth.\n",
    "                continue\n",
    "            classes.append((t, f\"{prefix}.{token}\"))\n",
    "        else:\n",
    "            # Ignore all others.\n",
    "            continue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the set of unique function names.\n",
    "f_set = set([f.split(\".\")[-1] for f in functions])\n",
    "len(f_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To reduce false positives from other libraries, in in the function .sum(\n",
    "# we block numpy and matplotlib prefixes. Then convert to regex tokens.\n",
    "\n",
    "blocked_prefixes = \"(?<!numpy|np|plt|matplotlib)\"\n",
    "function_token_set = {f\"{blocked_prefixes}\\.{f}\\(\" for f in f_set}\n",
    "indexer_token_set = {f\"\\.{indexer}\\[\" for indexer in indexers}\n",
    "pandas_token_set = {\"pd\", \"pandas\"}\n",
    "search_tokens_set = function_token_set | indexer_token_set | pandas_token_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the helper we apply to the DataFrame to parse a script for its tokens.\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def parse_file_tokens(file_path, search_tokens):\n",
    "    \"\"\"Parse the file and search for the desired regex expressions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    file_path: str\n",
    "        File path to search for.\n",
    "    search_tokens: str\n",
    "        Regex expression as a string.\n",
    "    \"\"\"\n",
    "    with open(file_path) as f:\n",
    "        contents = f.read()\n",
    "    search_tokens = regex.compile(\"|\".join(search_tokens))\n",
    "    return Counter(regex.findall(search_tokens, contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a DataFrame of the python scripts.\n",
    "\n",
    "python_scripts = []\n",
    "scripts_dir = \"../data/big_dataset/converted_scripts/\"\n",
    "\n",
    "for f in os.listdir(scripts_dir):\n",
    "    if not f.startswith(\".\") and not f.endswith(\"csv\"):\n",
    "        python_scripts.append(os.path.join(scripts_dir, f))\n",
    "\n",
    "python_scripts_df = mpd.DataFrame(python_scripts, columns=[\"script_path\"])\n",
    "python_scripts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use parse script to get a Counter of the tokens in each script.\n",
    "\n",
    "python_scripts_df[\"script_tokens\"] = python_scripts_df.apply(\n",
    "    lambda path: parse_file_tokens(path[\"script_path\"], search_tokens_set), axis=\"columns\"\n",
    ")\n",
    "\n",
    "python_scripts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_scripts_df.to_csv(\"../python_script_tokens_df.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process the token counters to DF columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_scripts_df = mpd.read_csv(\n",
    "    \"../python_script_tokens_df.csv\"\n",
    ")  # Given that we have a column of Counter objects, we now want to expand them to their own columns, for each token.\n",
    "python_scripts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the display names of all search tokens.\n",
    "function_token_set = {f\".{f}(\" for f in f_set}\n",
    "indexer_token_set = {f\".{indexer}[\" for indexer in indexers}\n",
    "pandas_token_set = {\"pd\", \"pandas\"}\n",
    "all_tokens = function_token_set | indexer_token_set | pandas_token_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_token(row: str, token):\n",
    "    row = ast.literal_eval(row)\n",
    "    try:\n",
    "        return row[token]\n",
    "    except Exception:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in all_tokens:\n",
    "    python_scripts_df[token] = python_scripts_df[\"script_tokens\"].apply(\n",
    "        lambda row: get_token(row, token)\n",
    "    )\n",
    "python_scripts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_scripts_df.to_csv(\"../token_breakdown.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform post-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm all column names are unique\n",
    "assert all(\n",
    "    [count == 1 for f, count in Counter(python_scripts_df.columns).items()]\n",
    "), \"Column names should be unique.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove irrevelant tokens\n",
    "\n",
    "Like Int16Dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all the possible functions from base pandas class, dataframes, and series.\n",
    "\n",
    "# Also add an exclusion set.\n",
    "\n",
    "search_tokens_set = set()\n",
    "exclude = set()\n",
    "objects = [pd, pd.DataFrame, pd.Series]\n",
    "indexers = [\"iloc\", \"iat\", \"ix\", \"loc\", \"at\"]\n",
    "for obj in objects:\n",
    "    for token in dir(obj):\n",
    "        # We do not consider private functions or properties\n",
    "        if token[0] == \"_\" and token[:2] != \"__\":\n",
    "            continue\n",
    "        elif inspect.isfunction(getattr(obj, token)):\n",
    "            # For functions, we search for \".function_name(\"\n",
    "            continue\n",
    "        elif token in indexers:\n",
    "            # For indexing functions, we searhc for \".indexing_function[\"\n",
    "            continue\n",
    "        else:\n",
    "            # For properties, we add only a period in front\n",
    "            exclude.add(\".{}\".format(token))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional tokens to exclude.\n",
    "exclude = exclude.union(\n",
    "    [\".datetime\", \".base\", \"pandas\", \"pd\", \"pd.DataFrame\", \"pd.dataframe\"]\n",
    ")\n",
    "exclude = exclude.intersection(set(python_scripts_df.columns))\n",
    "print(len(exclude))\n",
    "exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove these tokens from the df.\n",
    "python_scripts_df.drop(labels=exclude, inplace=True, axis=1)\n",
    "python_scripts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove rows with no tokens.\n",
    "python_scripts_df[\"token_count\"] = python_scripts_df.apply(\n",
    "    lambda row: row[3:].sum(), axis=1\n",
    ")\n",
    "python_scripts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_scripts_df.drop(\n",
    "    python_scripts_df[python_scripts_df[\"token_count\"] == 0].index, inplace=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_scripts_df.drop(columns=[\"token_count\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_scripts_df = python_scripts_df.drop(columns=[\"Unnamed: 0\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "python_scripts_df.to_csv(\"../data/filtered_token_breakdown.csv\",index=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
